---
title: Metrics and adequacy criteria
category: definition
author: Andrey Sadovykh
layout: page
---

# Intro

(D. Hubbard, How to Measure Anything, 2010) Everything is measurable

If X is something we care about, then X, by definition, must be detectable.
  How could we care about things like “quality,” “risk,” “security,” or “public image” if these things were totally undetectable, directly or indirectly?
  If we have reason to care about some unknown quantity, it is because we think it corresponds to desirable or undesirable results in some way.
If X is detectable, then it must be detectable in some amount. 
  If you can observe a thing at all, you can observe more of it or less of it
  If we can observe it in some amount, then it must be measurable.


Example for discussion for quality measurement. Garvin's tansendental view. Discuss example of compering Manet and Picaso artwork. Which one is better (more quality)?

# How does this looks for Software Quality

Example below for ISO 25002 Attributes-metrics mapping example

![Attributes-metrics mapping example](http://www.plantuml.com/plantuml/png/RLXjSjou4VtlKtHLeggrQosDlLOYgrAfbLPIhIghsD67_qDW3mSn2J3enaZZ6-GUkK2kaAFa90iqcn-Kvzy83Ju-l7xTQEfdOXEvTRvgJVVg3VmZcSRn3iwOetjCZ0G_NDzzWlV7gzNxrw_Ul86hv2sxu6LVNdW3TycnUNJwTrxu9SI8bfTRJ-40mXeOYS4QGeBE4645F-1XVVWZV3m-U3qytdQDCzuYF3dUp63W5l-LNCGMFTCf_A40jex8-HhMQ3X5f05lzLr4uE7CsXIDLQgV8rf76Rn0tIAbfW1VuDOPBglh65h0vmujOmLRqoGSNl2__wuEL-yWUSSqClVaYD7RtGBlYucAvpn4xD0Kj92uIpmI_ilxNW2uzemnT1XLr4cLZD4lzS9SFZF68ilTZXZRGH7d6Uke_8rhvggyEtgtccyWsQ7qBmoR36etnjSu2gM9gqgGe_6qnXYn0YDnncOtfl3ZuKSzmvJ37lgisc34ab8ESCV6LoPMLA3jYsndjnrTN_u2f9icwglLu91Rh5DZECJTx4Lw-ZZ5jYuqSih953sxEypKLLbn11ALv74CJU6Kl4cy2T5zatDAs31Zsp515t6Bdh6Q4riew6tSTUMzft3k61hqDMyADnmZa2Rhv-Bwqzj7nt1sUQSuNrBZn_p3ZV6c5jEVmhrhHSE859ej0kbHO3jxnE7agWydi0afPek-4kKQDxDLXkQP4bUzM-3MCAgR9WTr6A4DdZSZxJ5s9ElA_Q7Zml4pHekUCt4ra3XQwrXsp5mZjX42gM6u3mt6oJkNCOQOcTkmnRWJ1KxX1eq77iMGc5PhsJwTF_kTSjXNMgHQB1FUNOTOpOMLbWXcpHZrX2FNdvPW2h8ir7DZlwOEZAzoc3X4LChZ6tJLm3er3JpA_abKoHQiGuHtz-U3hnmPOfnKAFBcBMrl_8E-EbK6h9HyfK_Db3uvw7tmG7CNFFWAenJS_2GZuT_GnEpbNuLOx5jmLG8SZVDWmECJ99fUz44jfvshCT6nTCIBO5FniVX0AesmO1ekZRgQHy7gOt_zS7Gyda0hhQUq95ku59H9x0LNOpSEjmWdgmFeObWxwwHF3vbTKFwR3qyDLZLcmHczifNxoz1NwfiMSaQs_LtNx5Jpz_24-7Mqtycn4lxqifzV6i82-7UpTRNg8GLd6sSp08VnBM3AppPuUVi0_Vz__-SdtQoixqjK6Y8MF5NYh1YqHYcM1UfNtEbnzhjRXW1ZsMtH8v5MDs7iRwYriGWNIAxsS2i9_2wX3zVBxAaOYd9ds5564QvtAMvnHnDTcaV9XeYZkmx_V7JSs_XoFUJnrMZOSznaIKCShWhgWgUNDTZ5q7TryGWIJ_r2DHWqtSIGwasNMBhYQGQK_GGRuwi6gz8NzP6nXbUh0PQEScZe5HHIj0WFx1htJLAk1VBveHHaFILBl2RhIJPZDUKeXpnyU2kAaFGofNg3RNyZUagjt38JNYknnQrpd5dUdEjrRJfZHslUc6VNfXP-2wQ2Nyin9Rbv6IW76ZOUGoBDDQMk2t5n7_wuMgqaElPSQvhnDaD3AQ9S_jtKnQ32Efggh_tagr9Bpz1a2cDgZhLvtHZdsRXv_SEPDytkswB2Bl0nrNNnuJBtJR16Ygh2DTfPvgK5l9vVEjTLAVxiCQ6b3f3Sw2tZT0DMKAKeJMnyw-Hw5LdcgWN5cr1Dw4kMBbUftFbWvJaQUUeLKSJJc78-gqp7WbKA8qG-pMOWBdHD2HLI7AHRnscG7e7_c1qXe3emwwp42yPNvSbfmAqZaepFvwccSG8RLxCK1Cvp35MoaqDM4X3yGHQuRZjZkSyZENevVZe9puVEBcBbxCnt7qzKw7ka1MbPo5BqjQ20DbG8mttvk0EJEBIcm34IBngugYiRHDqhDAIJ-BwRHf70J5yxinRREfyOTHxx7E94TKpXSkxtQrUdA6_EyNycOh75qYwIOFaX-KM_TJ5usJYK9LHcfsUvQl1P3o3NCdPDs9NDciBPutiA_7WH7pvQY4tJBAZhsbvtzo6WiEOd4t4aEUcp1qEASgn2DmkHV3nyD_kDNoDlytc9KMVAgHdBr8z-KTrnYDaiVUCi5mi3RKp4QgdCyijj_qVQ9zvW7B_Gn7lQjfVABdl5BNQDiRXtOj3OY-y1MTHHeGi-1qTwa-dsqbn2Ib2ULxO3z7upwDVFp-Wh_I_2xm00)

*Figure. Mapping ISO 25002 Software Quality Attributes to Metrics*

Consider the example. For the attributes we need metrics and measurements to analyse the degree to which the QA is achieved or present in the developped/released systems.

Metrics are usefull means to control quality during the development and when accepting the release. Companies set goals and quality gates at various stages of product lifecicle using specific metrics.

We need to learn to setup the right metrics in the right way.


#  Learning goals

Define measurement and metrics, specifically in the context of software quality.
Enumerate several ways measurement can go wrong (so you can avoid them!).
Evaluate measurement data in terms of scale, precision, reliability, validity, etc.
Describe mechanisms to develop a (valid!) metric or measurement for your quality concerns. 
Evaluate new metrics with a critical eye.
Enumerate approaches to setting up a successful metrics program in your organization (bearing in mind potential risks). 

# Definitions
Software quality metric. IEEE 1061 says:
“A software quality metric is a function whose inputs are software data and whose output is a single numerical value that can be interpreted as the degree to which software possesses a given attribute that affects its quality.” 

Measurement 

Def by (D. Hubbard, How to Measure Anything, 2010)

Measurement: a quantitatively expressed reduction of uncertainty based on one or more observations.
Definition:
Measurement of some attribute of a set of things is the process of assigning numbers or other symbols to the things in such a way that relationships of the numbers or symbols reflect relationships of the attributes of the things being measured. (W. Sarle, 1997)

Measurement basics
What does the measurement represent?
How accurate is the measurement?
How precise is the measurement?
What is the resolution of the measurement?

# What can go wrong with metrics

![Dilbert gets rich](http://dilbert.com/strips/comic/1995-11-13/
)

In summary:
Bad statistics: A basic misunderstanding of measurement theory and what is being measured.
Bad decisions: The incorrect use of measurement data, leading to unintended side effects.
Bad incentives: Disregard for the human factors, or how the cultural change of taking measurements will affect people

The Dangers of Using Software Metrics to (Mis)Manage 
There sometimes is a decidedly dark side to software metrics that many of us have observed, but few have openly discussed. It is clear to me that we often get what we ask for with software metrics and we sometimes get side effects from the metrics that overshadow any value we might derive from the metrics information. Whether or not our models are correct, and regardless of how well or poorly we collect and compute software metrics, people’s behaviors change in predictable ways to provide the answers management asks for when metrics are applied. I believe most people in this field are hard working and well intentioned, and even though some of the behaviors caused by metrics may seem strange, odd, or even silly, they are serious responses created in organizations because of the use of metrics. Some of these actions seriously hamper productivity and can effectively reduce quality (“The Darker Side of Metrics,” Eighteenth Ann. Pacific Northwest Software Quality Conf., Pacific Northwest Software Quality Conf., Portland, Ore., 2000, pp. 265-272; http://www.pnsqc. org/hot/proceedings.htm). 

## Metrics biases and falacies

Examples:
Streetlight effect
McNamara Fallacy
Wrong correlations
Confounding variables, 
wrong representation.

# Are your measurements right ?

## Representation condition
A numerical model should make sense in terms of the real-world entity it describes.
More formally: a measurement should map real world entities into numbers and real-world relations into numerical relations such that the real-world relations are preserved in the numerical relations. 
Example: LOC satisfies the representation condition for physical program size, but not functional program size.

Resolution, accuracy, precision of your measurements

## Scale
Scale: the type of data being measured.
The scale dictates what sorts of analysis/arithmetic is legitimate or meaningful.
Your options are:
Nominal: categories
Ordinal: order, but no magnitude.
Interval: order, magnitude, but no zero.
Ratio: Order, magnitude, and zero.
Absolute: special case of ratio.

For example, software testers often use a ratio to represent the number of defects found to the number of defects fixed during testing. A ratio greater than one during a specified time period, indicates that developers are discovering problems faster than they can fix them. When the ratio is below one, developers are reducing the number of known problems. An example of a nominal value would be a case where developers assign priority or severity levels to defects, entering this information into a defect-tracking database for software testing.
Ordinal values can represent a position, such as when an athlete finishes a race first, second, or third. You must preserve the ratio relationship—by not mixing ratio values with ordinal values, for example—to make measurement meaningful.

## Understanding data

### Mean, median and mode

What is the difference between “average” and “mean”? An average is a single value that is meant to be representative of a set of values, as such the mean, the median and the mode are all averages, but be careful because this is not the common usage where average tends to be equated with arithmetic mean.

Geometric mean: which indicates the central tendency or typical value of a set of numbers by using the product of their values (as opposed to the arithmetic mean which uses their sum). The geometric mean is defined as the nth root (where n is the count of numbers) of the product of the numbers. A geometric mean is often used when comparing different items – finding a single "figure of merit" for these items – when each item has multiple properties that have different numeric ranges.[1] For example, the geometric mean can give a meaningful "average" to compare two companies which are each rated at 0 to 5 for their environmental sustainability, and are rated at 0 to 100 for their financial viability. If an arithmetic mean was used instead of a geometric mean, the financial viability is given more weight because its numeric range is larger- so a small percentage change in the financial rating (e.g. going from 80 to 90) makes a much larger difference in the arithmetic mean than a large percentage change in environmental sustainability (e.g. going from 2 to 5). The use of a geometric mean "normalizes" the ranges being averaged, so that no range dominates the weighting, and a given percentage change in any of the properties has the same effect on the geometric mean. So, a 20% change in environmental sustainability from 4 to 4.8 has the same effect on the geometric mean as a 20% change in financial viability from 60 to 72.

In mathematics, the harmonic mean (sometimes called the subcontrary mean) is one of several kinds of average. Typically, it is appropriate for situations when the average of rates is desired.

The harmonic mean is one of the three Pythagorean means. For all positive data sets containing at least one pair of nonequal values, the harmonic mean is always the least of the three means, while the arithmetic mean is always the greatest of the three and the geometric mean is always in between. (If all values in a nonempty dataset are equal, the three means are always equal to one another; e.g. the harmonic, geometric, and arithmetic means of {2, 2, 2} are all 2.)


### Range, standard deviation, skewness, kurtosis

Standard deviation shows how much variation or dispersion exists in a dataset. For a finite set of numbers, the standard deviation is found by taking the square root of the average of the squared differences of the values from their average value. 

Skewness measures the degree of asymmetry of a distribution. A value of 0 means a symmetric distribution, while a positive value means the distribution is skewed to the right. The largest the skewness the longer the tail of the distribution.

Kurtosis is a measurement of the flatness of an approximately symmetric distribution. The “reference” of kurtosis is given by the normal distribution for which its value is 3. A distribution which kurtosis higher than 3 is less peaked than the Normal distribution.  A distribution with kurtosis lower than 3 is more peaked than a Normal distribution.  The kurtosis of a uniform distribution is 4.2. Some authors use what is called the Fisher kurtosis which is identical to the other except that the value of 3 is subtracted to have 0 as a reference for a Normal distribution.


### Correlations

Correlation is a general term that describes whether two variables are associated. The term associated means ‘go together’, that is, knowing the value of one variable, X, enables better prediction of a correlated (associated) variable, Y.
Correlation does not imply causality. Height and vocabulary of children are correlated – both increase with age. Clearly, an increase in height does not cause an increase in vocabulary, or vice versa.
To see whether two variables are associated or not the best instrument is the scatter plot. 

A scatter plots reveals relationships or association between two variables. Such relationships manifest themselves by any non-random structure in the plot. Scatter plots help answer the following questions: 
Are variables X and Y related? 
Are variables X and Y linearly related? 
Are variables X and Y non-linearly related? 
Does the variation in Y change depending on X? 
Are there outliers?
Are there influential points?

#### Problems with correlations

Confounding variables. 

For example. Khaled El Emam, Saida Benlarbi, and Nishith Goel ,September 1999
They say that : Only four, out of twenty-four commonly used object-oriented metrics, were actually useful in predicting the quality of a software module when the effect of the module size was accounted for. 


Other causes of spurious correlation:
Both variables change with time
Response and explanatory variable are swapped in our model. 
To substantiate causation:
Provide a theory (from domain knowledge, independent of data)
Show correlation
Demonstrate ability to predict new cases (replicate/validate)


### Significance

Statistical significance: the probability that an observed effect is not due to random chance.
Blindly computing statistics without considering the significance of the results is dangerous.
Short of sophisticated statistics, increase confidence in your claims by sanity checking your data.


# How to develop metric or measurement

## Measurements validity

  Construct – Are we measuring what we intended to measure?
  Predictive – The extent to which the measurement can be used to explain some other characteristic of the entity being measured
  External validity – Concerns the generalization of the findings to contexts and environments, other than the one studied

## Basili’s Goal, Question, Metrics

Conceptual: [Goal] defines the objectives (scope, purpose) to be addressed by measurement. 
Operational: [Question] defines what quantifiable information is required to determine whether there is progress toward the goal(s). 

Question: Provide a focus for measure and metrics by defining required attributes and characteristics. Additional questions can address and minimize potential side effects of collected measures. Often based on models

Quantitative: [Metric] defines the particular attributes, definitions, and observation (collection) frequency for measurement data. 
Determine measurement theory, statistical design, and applicability of measurement data at this level. 


## Operational definition

* An operational definition defines a characteristic in terms of how it is measured. Two important criteria (Park 92):
Communication: If someone uses the definition, will others know precisely what has been measured, how it was measured, and what was included/excluded?
Repeatability: Could others use the definition to repeat the measurements and get essentially the same results?
* Important definitions:
Software Quality Measurement: A Framework for Counting Problems and Defects (CMU/SEI-92-TR-22, ADA258556)
Software Effort and Schedule Measurement: A Framework for Counting Staff-Hours and Reporting Schedule Information (CMU/SEI-92-TR-21, ADA258279)
Software Size Measurement: A Framework for Counting Source Statements (CMU/SEI-92-TR-20, ADA258304

## Measurement development process

Adapted from: Class and Object Modularity Description and Measurement, C. McLean, 2006


Stage 0, GQM. Why do I need to measure this

Stage 1 is the conceptual definition stage in which the characteristic to be described by the measure is defined "in terms of other concepts, the meaning of which is assumed to be more familiar to the reader.“ The conceptual definitions "guide the development" of the operational measure definitions. If the characteristic is complex, it may not be possible to conceptually define it in simple, familiar terms. In this case, the characteristic must be initially defined in terms of the sub-characteristics that contribute to its manifestation in the software

Stage 2 of the systematic descriptive measure development process is operational definition. The operational definition defines a characteristic in terms of how it is to be measured. The level of measurement achieved by each measure should be determined and stated for each measure. Figure 1-3 also shows that validation is part of the operational definition stage of measure development. This particular validation assesses the degree to which the defined measures provide an adequate description of the features.

Stage 4 of the systematic descriptive measure development process is measurement instrument implementation. In this stage, the measures operationally defined in the previous stage are implemented within a measurement instrument. It is uncommon to specifically discuss measurement instrument implementation as part of the measure development process. Many measure development "approaches stop at the point at which measurement concepts or specific metrics are identified. They do not define how such measures can be collected and stored, nor (in general) do they define how they can be analyzed." (Kitchenham, Hughes & Linkman 2001, p. 788). Measurement instrument implementation is included as part of this measure development process because implementing measures that provide a detailed description of a complex software characteristic can be problematic. The large amounts of data required to formulate a detailed measured description of a software characteristic can be difficult to extract from a software document and may require large amounts of storage and complex manipulations. Addressing some of these issues explicitly in the measure development process
will help a future user of the defined measures implement them within their own selected measurement instrument. Stage 4 measurement instrument implementation is described and demonstrated in Chapter 6 of this thesis. 

Although Figure 1-3 shows the systematic measure development process as being linear, like many development processes, it may be necessary to return to previous stages to clarify or improve on the work conducted at that stage. These reiterations are not shown specifically on Figure 1-3 however they can occur from any stage back to any previous stage. Once changes have been made to a stage, it is necessary to update all the following stages to take into account the changes made.

Validity is “the extent to which any measuring instrument measures what it is intended to
measure.”

Reliability “concerns the extent to which an experiment, test, or any measuring procedure
yields the same results on repeated trials” (Carmines & Zeller 1979, p. 11). It is “the degree to
which an instrument measures the same way each time it is used under the same conditions
with the same subjects.”

Measures can be classified according to the level of measurement they achieve (Stevens 1946).
The level of measurement achieved by a measure dictates the mathematical manipulations and
statistical analysis that can be performed on the collected measure data

## Also with SIX STEPS

Develop
  Develop a set of project business and measurement goals for productivity and quality
Generate
  Generate questions (based on models) that completely and quantifiably define those goals.
Specify
  Specify the measures to be collected to answer those questions and track process and product conformance.
Develop
  Develop mechanisms for data collection
Validate and analyze
  Collect, validate and analyze the data in real-time to provide corrective, actionable project feedback.
Analyze
  Analyze the data in post mortem to assess conformance and make recommendations for future improvements

 ## Example. McCall Model Mapping

![McCall Use-Factor-Criteria-MetricsMap](http://www.plantuml.com/plantuml/png/ZL9jQiCm3FtVK_Xt87VeQ3SOhAoqtG4yvMK872T8Ic6tdw2XhYc4_KY8NfxUX_5MBOeDdBiXJfic76WNKmfVYlOjaetIxeGDmh4zm8H9DqqJZZ9sCrdud23HUCmEDhuKlpcn_VhausuSXZapEU6A3DKR_4BatroOuOJ4rQPJPebqrydAQiXKXAS4ksk6rxvduaBOuyg49xXsVknnyWLTQlWmwqgSiuclp8AkTFA8n8e2B0dUSuS9_ig4suyF_F2ZzXcfR_TG4fxgSxgu9MBXXaFaRFuKxBzfQjDm7CMAI7sWw_d31Mhh_hNTSvEjootNxGy0)

*Figure. McCall Use-Factor-Criteria-Metrics Mapping*

## Example. Metric Decomposition.

![Metrics Decomposition Example](http://www.plantuml.com/plantuml/png/LP31QiCm44Jl-GgTVUal1De4UYY493-mjSRkWhGogrL9-_ML8eFhWozlPhoZEMOZjSZY8os7mNqGYzMFFZcm_Ho6mRqcLOosaS6TgGJBLIbY3LHJIBaet9qZEddFAP3XvSoFZVQakvBX-QFJD2MrBbsHKz4HxgBmF1edwK8tkTDZWNYsecYrxiYxJc-O5N1fUYeiSm_VZ0mHOhNjDvJc5bxZxX98AezBW46GyxvKEqdYroixhJtvYsH67w45DzHDEtJZN_m7VO8ZnA_R_m40)

*Figure. Metrics Decomposition*

## Example. Object-oriented metrics

Number of Methods per Class
Depth of Inheritance Tree
Number of Child Classes
Coupling between Object Classes
Calls to Methods in Unrelated Classes


# Discussion. Maintainability Index

Definition.

Maintainability Index = 
MAX(0,(171 – 
      5.2 * log(Halstead Volume) – 
      0.23 * (Cyclomatic Complexity) – 
      16.2 * log(Lines of Code)
      )*100 / 171)

Origins:
1992 Paper at the International Conference on Software Maintenance by Paul Oman and Jack Hagemeister

Developers rated a number of HP systems in C and Pascal
Statistical regression analysis to find key factors among 40 metrics

Question: how good it is for code in C, C++, Java, Python, TypeScript?

Microsoft says:

Design rationale (from MSDN blog)

"We noticed that as code tended toward 0 it was clearly hard to maintain code and the difference between code at 0 and some negative value was not useful."
"The desire was that if the index showed red then we would be saying with a high degree of confidence that there was an issue with the code."

![Source. MSDN blog](https://learn.microsoft.com/en-us/visualstudio/code-quality/code-metrics-maintainability-index-range-and-meaning)

# Examples of commonly used automated metrics

Development time:
  - lines of code
  - test coverage (line, branch, etc.)
  - cyclomatic complexity and its direvatives  like maintainability index
  - anherence to code styles


Performance:
  - response time, throughput, utilisation under different types of load patters (normal, stress, spikes, etc.)

Reliability:
  - mean time to failure
  - mean time to repair
  - mean time between failures

Security:
  - code reviews and inspections for anherence to security design practices (least privelege, hash/salt, input sanitation)
  - passage of analysis checks by tools (bandit, snyk, etc.)


Maintainability:
  - cyclomatic complexity and its direvatives like maintainability index
  - complex tech debt metrics (SQALE, SONAR and others)
  - dependancy analysis and change impact with Design Structure Matrix and other methods 



